# ğŸ¤– Nemotron AI Voice Assistant

A **high-performance, self-hosted AI voice assistant** powered by **NVIDIA Nemotron** models, featuring:

* ğŸš€ **Multi-Backend LLM Support** - Switch between GGUF (llama-cpp), vLLM, and HuggingFace
* ğŸ”Š **NVIDIA Magpie TTS** - High-quality multilingual speech synthesis with 5 voices
* ğŸ“Š **NeMo FastPitch + HiFi-GAN** - Ultra-low latency fallback TTS
* ğŸ¤ **Real-time ASR with NVIDIA Canary-1B-Flash** - State-of-the-art speech recognition
* ğŸ‘ï¸ Vision understanding with BLIP
* ğŸŒ **Multi-Source Search** - Google, Britannica, and Academia integration
* ğŸŒ¤ï¸ Context-aware weather & time
* ğŸ§  Optional "Deep Think" mode with separate reasoning display
* âš¡ Optimized for **dual-GPU** setups (Ada + Volta)

Designed for **local execution**, **full control**, and **maximum performance**.

**ğŸŒ Live Demo:** [nemotron.burtoncummings.io](https://nemotron.burtoncummings.io)

---

## âœ¨ Key Features

| Feature                         | Description                                               |
| ------------------------------- | --------------------------------------------------------- |
| ğŸ¨ **Matrix-style Web UI**      | Animated cyber-themed interface, fully responsive         |
| ğŸ¤ **Voice Input**              | Push-to-talk or continuous listening                      |
| ğŸ—£ï¸ **Streaming Voice Output**  | Sentence-level TTS while the model is still thinking      |
| ğŸ§  **Deep Think Mode**          | Displays internal reasoning separately from spoken answer |
| ğŸ”„ **Multi-Backend LLM**        | GGUF, vLLM, or HuggingFace - your choice                  |
| ğŸ”Š **Magpie TTS**               | 5 HD voices, 7 languages, natural speech                  |
| ğŸ‘ï¸ **Vision / Image Analysis** | BLIP image captioning                                     |
| ğŸŒ **Multi-Source Search**      | Google, Britannica Encyclopedia, Academia                 |
| ğŸŒ¤ï¸ **Weather Awareness**       | OpenWeather API                                           |
| ğŸ“Š **Performance Metrics**      | Live latency stats via `/metrics`                         |
| âš¡ **Multi-GPU Optimized**       | Separate GPUs for realtime vs batch tasks                 |
| ğŸ“± **Mobile Responsive**        | Fully optimized for phones and tablets                    |

---

## ğŸ–¥ï¸ Hardware Requirements

### Minimum

| Component | Requirement                    |
| --------- | ------------------------------ |
| GPU       | NVIDIA GPU with **12GB+ VRAM** |
| RAM       | 16GB                           |
| Python    | 3.10+                          |
| CUDA      | 13.x                           |
| Driver    | 580.x (January 2026+)          |

### Recommended (Dual GPU)

| Component                    | Purpose                         |
| ---------------------------- | ------------------------------- |
| **GPU 0 â€” RTX 4060 Ti 16GB** | LLM, NeMo TTS                   |
| **GPU 1 â€” TITAN V 12GB**     | ASR, Vision, Magpie TTS, Canary |
| CPU                          | Modern 8â€“16 core                |
| RAM                          | 64GB                            |
| CUDA                         | 13.0                            |
| Driver                       | 580.120                         |

### Approximate VRAM Usage

```
GPU 0 (RTX 4060 Ti 16GB):
- Nemotron LLM (GGUF Q4)     ~5.0 GB
- NeMo FastPitch + HiFi-GAN  ~0.5 GB
- CUDA overhead              ~1.0 GB
------------------------------------
Total                        ~6.5 GB

GPU 1 (TITAN V 12GB):
- NVIDIA Canary-1B-Flash     ~2.5 GB
- Magpie TTS                 ~3.8 GB
- BLIP Vision                ~2.8 GB
- CUDA overhead              ~1.0 GB
------------------------------------
Total                        ~10-11 GB
```

---

## ğŸ§  Model Stack

| Component     | Model                                      | Purpose                       |
| ------------- | ------------------------------------------ | ----------------------------- |
| ASR           | `nvidia/canary-1b-flash`                   | Real-time speech-to-text      |
| LLM           | `nvidia/NVIDIA-Nemotron-Nano-9B-v2`        | Language reasoning & response |
| LLM (GGUF)    | `nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M.gguf` | Fast local inference     |
| TTS (Primary) | **Magpie TTS** (357M multilingual)         | HD quality, 5 voices          |
| TTS (Fast)    | NeMo FastPitch + HiFi-GAN                  | Ultra-low latency fallback    |
| Vision        | BLIP                                       | Image captioning              |

---

## ğŸ”„ LLM Backend Options

The server supports **three LLM backends** - switch with a single config change:

| Backend | Config Setting | Speed | VRAM | Best For |
|---------|----------------|-------|------|----------|
| **GGUF** | `llm_backend_preference = "gguf"` | ~15-25 tok/s | ~5GB | Local files, efficiency |
| **vLLM** | `llm_backend_preference = "vllm"` | ~20+ tok/s | ~10GB | Maximum speed |
| **HuggingFace** | `llm_backend_preference = "hf"` | ~5 tok/s | ~6GB | Compatibility |

### Switching Backends

Edit `ServerConfig` in the code (around line 453):

```python
# Use local GGUF file (recommended)
llm_backend_preference: str = "gguf"

# Or use vLLM for maximum speed
llm_backend_preference: str = "vllm"

# Or use HuggingFace 4-bit fallback
llm_backend_preference: str = "hf"
```

### Using Different GGUF Models

```python
# Default Nemotron 9B Q4
gguf_model_path: str = "models/nemotron-9b/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M.gguf"

# Or use a different quantization
gguf_model_path: str = "models/nemotron-9b/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_M.gguf"

# Or a completely different model (must be compatible with chat template)
gguf_model_path: str = "models/llama-3/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
```

### GGUF Settings

```python
gguf_n_ctx: int = 4096          # Context window size
gguf_n_gpu_layers: int = -1     # -1 = all layers on GPU
gguf_n_batch: int = 512         # Batch size for prompt processing
gguf_verbose: bool = False      # Verbose llama.cpp output
```

---

## ğŸ”Š TTS Options

### Magpie TTS (Primary - High Quality)

```python
tts_engine: str = "magpie"  # HD quality, 5 voices, 7 languages
```

**Available Voices:**
- `Sofia` - Female, Warm (default)
- `Aria` - Female, Expressive
- `John` - Male, Professional
- `Jason` - Male, Casual
- `Leo` - Male, Friendly

**Supported Languages:** English, Spanish, French, German, Italian, Vietnamese, Chinese

### NeMo FastPitch (Fallback - Ultra-Fast)

```python
tts_engine: str = "nemo"  # ~50ms latency, single voice
```

---

## ğŸŒ Search Integration

### Multi-Source Search

The assistant supports multiple authoritative search sources:

| Source | Purpose | Use Case |
|--------|---------|----------|
| **Google Search** | General web search | Current events, general queries |
| **Britannica** | Encyclopedia | Factual information, definitions |
| **Academia** | Academic papers | Research, scientific queries |

### Configuration

```bash
# .env file
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CSE_ID=your_custom_search_id
BRITANNICA_API_KEY=your_britannica_key
```

---

## ğŸ§© System Architecture

```
Browser / UI (Mobile Responsive)
   â”‚
   â–¼
FastAPI Server (Uvicorn)
   â”œâ”€ ASR (Canary-1B-Flash)    â†’ GPU 1
   â”œâ”€ LLM (GGUF/vLLM/HF)       â†’ GPU 0
   â”œâ”€ THINK extraction
   â”œâ”€ Sentence streaming
   â”œâ”€ TTS (Magpie/NeMo)        â†’ GPU 0/1
   â”œâ”€ Vision (BLIP)            â†’ GPU 1
   â”œâ”€ Search (Google/Britannica/Academia)
   â””â”€ Metrics (/metrics)
```

---

## ğŸ“¦ Installation

### 1. System Requirements (January 2026)

```bash
# NVIDIA Driver 580.x (Jan 2026)
# CUDA Toolkit 13.0
nvidia-smi  # Verify driver version 580.x+
nvcc --version  # Verify CUDA 13.0
```

### 2. Python & CUDA

```bash
pip install torch torchvision torchaudio \
  --index-url https://download.pytorch.org/whl/cu130
```

### 3. Core Dependencies

```bash
# CUDA Toolkit 13 (if not installed)
wget https://developer.download.nvidia.com/compute/cuda/13.0.0/local_installers/cuda_13.0.0_580.51_linux.run
sudo sh cuda_13.0.0_580.51_linux.run --toolkit --silent --override

# Core packages
pip install fastapi uvicorn python-multipart websockets httpx aiofiles python-dotenv
pip install accelerate bitsandbytes transformers

# NeMo TTS (from main branch for Magpie support)
pip install "nemo_toolkit[tts]@git+https://github.com/NVIDIA/NeMo.git@main" --break-system-packages
pip install kaldialign --break-system-packages

# NVIDIA Canary ASR (replacing Whisper)
pip install nemo_toolkit[asr] --break-system-packages

# Alternative TTS options
pip install piper-tts --break-system-packages
pip install TTS --break-system-packages

# Audio processing
pip install soundfile librosa

# HTTP client
pip install httpx[http2]

# vLLM (optional - for vLLM backend)
pip install vllm

# Mamba dependencies (for Nemotron hybrid model)
pip install causal-conv1d --no-build-isolation --no-cache-dir
pip install mamba-ssm --no-build-isolation --no-cache-dir

# llama-cpp-python (for GGUF backend) - IMPORTANT: Build with CUDA 13
CMAKE_ARGS="-DGGML_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-13.0" \
  pip install llama-cpp-python --break-system-packages --force-reinstall --no-cache-dir
```

### 4. Download Models

```bash
# Create models directory
mkdir -p models/nemotron-9b

# Download GGUF model
# Options: Q4_K_M (~5GB), Q5_K_M (~6GB), Q8_0 (~9GB)

# Canary ASR model will auto-download on first use
```

---

## ğŸ”‘ Environment Variables

Create `.env`:

```bash
# Weather API
OPENWEATHER_API_KEY=your_key_here

# Google Search
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CSE_ID=your_custom_search_id

# Britannica Encyclopedia (optional)
BRITANNICA_API_KEY=your_britannica_key

# Academia Search (optional)
ACADEMIA_API_KEY=your_academia_key
```

All APIs are optional. The system runs fully offline without them.

---

## Quick Reference - Command Line Usage

### Basic Commands

```bash
# Default: Use GGUF with config settings
python3 nemotron_web_server_vllm.py --port 5050 --think

# Use vLLM backend
python3 nemotron_web_server_vllm.py --port 5050 --think --backend vllm

# Use HuggingFace 4-bit backend
python3 nemotron_web_server_vllm.py --port 5050 --think --backend hf
```

### Load Different GGUF Models

```bash
# Your default Nemotron model
python3 nemotron_web_server_vllm.py --port 5050 --think --backend gguf

# Load a specific GGUF model
python3 nemotron_web_server_vllm.py --port 5050 --think --gguf-model models/llama3/llama3-8b.Q4_K_M.gguf

# Load with custom context size
python3 nemotron_web_server_vllm.py --port 5050 --think --gguf-model models/mistral/mistral-7b.gguf --gguf-ctx 8192

# Load with specific GPU layers (for CPU/GPU split)
python3 nemotron_web_server_vllm.py --port 5050 --think --gguf-model models/big-model.gguf --gguf-layers 20
```

### TTS Options

```bash
# Use Magpie TTS with specific voice
python3 nemotron_web_server_vllm.py --port 5050 --think --tts magpie --voice John

# Use fast NeMo TTS
python3 nemotron_web_server_vllm.py --port 5050 --think --tts nemo
```

### List Available Models

```bash
# See what GGUF models are in your models/ folder
python3 nemotron_web_server_vllm.py --list-models
```

---

## ğŸ“‹ Arguments

| Argument | Values | Description |
|----------|--------|-------------|
| `--host` | `0.0.0.0`| Host to bind to |
| `--port` | `8000`| Port to bind to |
| `--reload` | `off`| Enable/Disable auto-reload for development |
| `--think` | `off`| Enable/Disable thinking/reasoning mode |
| `--stream` | `off`| Enable/Disable streaming mode |
| `--no-compile` | `off`| Disable torch.compile |
| `--backend` | `gguf`, `vllm`, `hf` | Choose LLM backend |
| `--gguf-model` | `path/to/model.gguf` | Path to GGUF file |
| `--gguf-ctx` | `2048`, `4096`, `8192`, etc | Context window size |
| `--gguf-layers` | `-1` (all), `20`, `30`, etc | GPU layers |
| `--tts` | `magpie`, `nemo`, `xtts`, `piper` | TTS engine |
| `--voice` | `Sofia`, `John`, `Aria`, `Jason`, `Leo` | Magpie voice |
| `--asr` | `canary`, `nemo` | ASR engine (default: canary) |
| `--list-models` | - | List available GGUF files |



---

## ğŸš€ Example Startup Output

```
======================================================================
ğŸš€ NEMOTRON VOICE AGENT v3.4
======================================================================
ğŸ§  LLM Backend:      GGUF (llama-cpp)
   Model Path:       models/nemotron-9b/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M.gguf
   Context Size:     4096
   GPU Layers:       -1 (-1 = all)
ğŸ¤ ASR Engine:       NVIDIA Canary-1B-Flash
ğŸ”Š TTS Engine:       Magpie TTS (HD quality)
   Default Voice:    Sofia
ğŸ§  Thinking Mode:    âœ… ENABLED
ğŸ“¡ Streaming Mode:   âœ… ENABLED (/ws/voice/stream)
âš¡ torch.compile:    âŒ DISABLED
ğŸŒ¤ï¸  Weather API:     âœ… CONFIGURED
ğŸ” Google Search:    âœ… CONFIGURED
ğŸ“š Britannica:       âœ… CONFIGURED
ğŸ“ Academia:         âœ… CONFIGURED
ğŸ“ Default Location: Branson, Missouri
ğŸ• Timezone:         America/Chicago
ğŸ’» CUDA Version:     13.0
ğŸ–¥ï¸  Driver Version:  580.120
======================================================================
```

---

## ğŸŒ Access Points

| URL                | Description                |
| ------------------ | -------------------------- |
| `/health`          | Server status              |
| `/metrics`         | Performance metrics        |
| `/chat`            | Text chat                  |
| `/chat/speak`      | Chat with TTS audio        |
| `/transcribe`      | Quick ASR (Canary)         |
| `/transcribe/file` | File transcription         |
| `/search/britannica` | Britannica search        |
| `/search/academia` | Academia search            |
| `/ws/voice`        | Standard WebSocket voice   |
| `/ws/voice/stream` | Real-time streaming voice  |

---

## ğŸ¤ Voice Interaction

### Push-to-Talk

1. Click **Record**
2. Speak
3. Release â†’ auto submit
4. AI responds with voice

### Streaming Mode (WebSocket)

* Tokens stream in real time
* Audio plays sentence-by-sentence
* Final response synthesized at completion

---

## ğŸ‘ï¸ Vision / Image Analysis

Upload or attach an image and ask:

> "What's in this image?"

The BLIP model analyzes and responds naturally.

---

## ğŸ§ File Transcription (Canary)

```bash
curl -X POST http://localhost:5050/transcribe/file \
  -F "file=@meeting.mp4"
```

Response:

```json
{
  "text": "Full transcription text..."
}
```

---

## âš¡ Performance Comparison

| Backend | Tokens/sec | First Token | VRAM |
|---------|------------|-------------|------|
| GGUF (Q4_K_M) | 15-25 | ~200ms | ~5GB |
| vLLM | 20+ | ~150ms | ~10GB |
| HuggingFace 4-bit | 5-8 | ~500ms | ~6GB |

| TTS Engine | Latency | Quality | Voices |
|------------|---------|---------|--------|
| Magpie | 3-8s | â­â­â­â­â­ | 5 |
| NeMo FastPitch | ~50ms | â­â­â­ | 1 |
| XTTS | 2-5s | â­â­â­â­ | Clone |
| Piper | ~100ms | â­â­â­ | Many |

| ASR Engine | Latency | Accuracy | Languages |
|------------|---------|----------|-----------|
| Canary-1B-Flash | ~150ms | â­â­â­â­â­ | Multi |
| NeMo Streaming | ~200ms | â­â­â­â­ | English |

> First request is always slower due to CUDA warm-up.

---

## ğŸ“ Project Structure

```
speechAi/
â”œâ”€â”€ nemotron_web_server_vllm.py   # Main server (v3.4 with multi-backend)
â”œâ”€â”€ nemotron_web_server.py        # Legacy backup
â”œâ”€â”€ nemotron_web_ui.html          # Web UI (mobile responsive)
â”œâ”€â”€ NemotronVoiceUI.jsx           # React UI component
â”œâ”€â”€ sw.js                         # PWA service worker
â”œâ”€â”€ manifest.json                 # PWA manifest
â”œâ”€â”€ README.md
â”œâ”€â”€ DOCS.md                       # API documentation
â”œâ”€â”€ .env
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ nemotron-9b/
â”‚   â”‚   â””â”€â”€ nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q4_K_M.gguf
â”‚   â””â”€â”€ magpie-tts/
â”‚       â””â”€â”€ magpie_tts_multilingual_357m.nemo
â””â”€â”€ static/
    â”œâ”€â”€ favicon.svg
    â”œâ”€â”€ og-image.png
    â””â”€â”€ apple-touch-icon.png
```

---

## ğŸ›  Troubleshooting

| Issue               | Fix                                      |
| ------------------- | ---------------------------------------- |
| Slow first response | Normal CUDA warmup                       |
| Out of VRAM         | Use GGUF backend, reduce `max_tokens_fast` |
| vLLM load fails     | Use GGUF or HF backend                   |
| GGUF not loading    | Check file path, reinstall llama-cpp-python with CUDA 13 |
| TTS errors          | NeMo fallback auto-used                  |
| Google timeouts     | API quota / network                      |
| Canary model error  | Update nemo_toolkit: `pip install -U nemo_toolkit[asr]` |
| CUDA 13 issues      | Verify driver 580.x: `nvidia-smi`        |

### Common GGUF Issues

```bash
# If llama-cpp-python wasn't built with CUDA 13:
CMAKE_ARGS="-DGGML_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-13.0" \
  pip install llama-cpp-python --force-reinstall --no-cache-dir

# If GGUF file not found:
# Check path in config: gguf_model_path: str = "models/nemotron-9b/your_model.gguf"
```

### CUDA 13 / Driver 580 Verification

```bash
# Check NVIDIA driver version
nvidia-smi

# Expected output should show Driver Version: 580.xxx

# Check CUDA version
nvcc --version

# Expected output should show release 13.0
```

---

## ğŸ“± Mobile Support

The web UI is fully responsive and optimized for mobile devices:

- **Touch-friendly** buttons and controls
- **Adaptive layout** for phones and tablets
- **PWA support** - install as app on mobile
- **Reduced animations** for battery saving
- **Portrait & landscape** orientation support

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Credits

* **NVIDIA Nemotron & NeMo** â€” NVIDIA
* **NVIDIA Canary ASR** â€” NVIDIA
* **llama.cpp** â€” ggerganov
* **vLLM** â€” UC Berkeley / community
* **BLIP** â€” Salesforce Research
* **FastAPI** â€” SebastiÃ¡n RamÃ­rez

---

<p align="center">
<b>Built for people who want AI on their own hardware.</b><br>
<i>Your AI â€¢ Your GPUs â€¢ Your Control</i><br><br>
ğŸŒ <a href="https://nemotron.burtoncummings.io">nemotron.burtoncummings.io</a>
</p>
